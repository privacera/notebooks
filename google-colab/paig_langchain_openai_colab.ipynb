{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9bucZb6kQFS"
      },
      "source": [
        "# Privacera Shield protecting LangChain Application\n",
        "This notebook shows how to use Privacera Shield Library with a LangChain application. To run this notebook you will need the following,\n",
        "\n",
        "\n",
        "1.  Sign up for a free account at [Privacera AI Governance (PAIG)](https://privacera.ai). This is simple, all you need is your email address.\n",
        "2.  OpenAI API Key. This will allow you to create your first OpenAI application governed by Privacera AI Governance. Please visit your [OpenAI account](https://platform.openai.com/settings/profile?tab=api-keys) to get this key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eFlXkpmH5MF"
      },
      "source": [
        "# Step 1 - Install the Python packages\n",
        "This step will take about a minute to complete.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w78AqwXxVG2D"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq \\\n",
        "  langchain-community==0.2.0 \\\n",
        "  langchain-openai==0.1.7 \\\n",
        "  langchain==0.2.0 \\\n",
        "  privacera_shield==1.1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhrfjKgIB2Q"
      },
      "source": [
        "# Step 2 - Upload Privacera AI application configuration file into Collab\n",
        "1. Log into your account in PAIG.\n",
        "2. To create a new application, go to `Application > AI Applications` and click the `CREATE APPLICATION` button on the right top. This will open a dialog box where you can enter the details of the application.\n",
        "3. Navigate to `Application -> AI Applications` and select the application you want to download the configuration file for. Click on the `DOWNLOAD APP CONFIG` button from the right top to download the configuration file.\n",
        "4. You will then upload the configuration file into the Collab notebook by following this step. After you run this step, click on Choose Files button and upload the AI application configuration file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4jAFpt3X69y"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "files = uploaded.keys()\n",
        "if len(files) > 1:\n",
        "  print(\"Upload only the application config json file\")\n",
        "else:\n",
        "  app_config_file_content = uploaded[list(files)[0]].decode('UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8GMQzsbIY8_"
      },
      "source": [
        "# Step 3 - Configure your OpenAI API Key into the environment.\n",
        "Enter your OpenAI API key in the pop-up. This key is stored in the environment of the notebook and not uploaded to PAIG.\n",
        "\n",
        "Hit enter after you have pasted the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "524RQuxcIjRd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key and hit Enter:\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8rihbIlUmyN"
      },
      "source": [
        "# Step 4 - Import all the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuy2qyBQUqwU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from privacera_shield import client as privacera_shield_client\n",
        "from privacera_shield.exception import AccessControlException\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# utility function to wrap the output\n",
        "def wrap_text(text, width=80):\n",
        "    words = text.split()\n",
        "    character_count = 0\n",
        "    for word in words:\n",
        "        if character_count + len(word) + 1 > width:  # Check if adding the word would exceed the width\n",
        "            print(\"\\n\", end=\"\")  # Start a new line\n",
        "            character_count = 0  # Reset the character count for the new line\n",
        "        print(word, end=\" \")  # Print the word followed by a space\n",
        "        character_count += len(word) + 1  # Update the character count\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOTTXIGbK2Do"
      },
      "source": [
        "# Step 5 - Initialize the Privacera Shield library\n",
        "This one line of code will initialize the Privacera Shield library so that it is ready to protect your LangChain application.\n",
        "\n",
        "> **Tip:** If you get an error in this step, it could be because you have already run this step once. You can ignore the error and continue. You can also try restarting the Kernel and start from the beginning with the proper JSON file and OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KfT9n58W0W9"
      },
      "outputs": [],
      "source": [
        "# Initialize Privacera Shield\n",
        "privacera_shield_client.setup(frameworks=[\"langchain\"], application_config=app_config_file_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF9i2B5CLAJS"
      },
      "source": [
        "# Step 6 - Initialize OpenAI based LLM object in LangChain\n",
        "In this step, we are creating an LLM object for the OpenAI LLM and a prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaYL9hvrLF2x"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")  # (1)\n",
        "\n",
        "llm = OpenAI(openai_api_key=api_key)\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l98gNiALR3c"
      },
      "source": [
        "# Step 7 - Run the LLMChain with your question\n",
        "1. In this step, we are going to run the LLMChain with the prompt asked by a user named `testuser`. \n",
        "2. Note how we are passing the username by creating a Privacera Shield context object. \n",
        "3. Privacera Shield will intercept the prompt and the response coming from LLM for the `testuser` and run policies. \n",
        "4. The PAIG service scans both the prompt and response text and runs security policies. \n",
        "5. If the policies decide that the access is denied then an AccessControlException is thrown.\n",
        "\n",
        "> **Note:** Here username used is `testuser` which is an external user. So the policies applied will be as per the public. For applying userspecific properties, create and use the user from PAIG portal `Account > User Management > User`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S56ozXYGJERe"
      },
      "outputs": [],
      "source": [
        "# Let's assume the user is \"testuser\"\n",
        "user = \"testuser\"\n",
        "prompt_text = \"Who was the first President of USA and where did they live?\"\n",
        "print(f\"Prompt: {prompt_text}\")\n",
        "print()\n",
        "\n",
        "llm_chain = prompt | llm\n",
        "try:\n",
        "    with privacera_shield_client.create_shield_context(username=user):\n",
        "        response = llm_chain.invoke(prompt_text)\n",
        "        print(f\"LLM Response:\")\n",
        "        wrap_text(response)\n",
        "except AccessControlException as e:\n",
        "    # If access is denied, then this exception will be thrown. You can handle it accordingly.\n",
        "    print(f\"AccessControlException: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozwXgZt3OArz"
      },
      "source": [
        "# Step 8 - Review the access audits in PAIG portal under Security menu option\n",
        "1. Now you can log in to the PAIG portal and check under `Security > Access Audits` section. You will see the audit record for the above run of your LangChain application.\n",
        "![title](img/Access_Audits.png)\n",
        "2. You can click on the eye icon and see the details of the prompts sent by the application to the LLM and the responses coming from the LLM.\n",
        "![title](img/Access_Audits_2.png)\n",
        "3. The default policy in PAIG for the application monitors the flow and tags the contents of the prompt and response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89FPYA9VxJ2"
      },
      "source": [
        "# Step 9 - Modify the policy of your AI application\n",
        "1. Log into the PAIG portal and navigate to your application under Application section. \n",
        "2. Click on the Permissions tab and enable the 'CCPA Personal Information Redaction' policy by clicking on the toggle in the status column. This will start redacting the sensitive data in the Prompt and Reply.\n",
        "![title](img/AI_App_Policy.png)\n",
        "3. Wait for few seconds and re-run Step 7 above. The reply will have the Location and Person name redacted, that is, replaced by `<<LOCATION>>` and `<<PERSON>>`. You can re-run Step 7 a few more times if the above is not redaction is not reflected.\n",
        "4. Then check the access audits and you will see the original response from the LLM and how Privacera AI redacted the sensitive information out of it.\n",
        "5. You can also modify the policy by clicking on the pencil icon and change the Prompt to Allow while keeping Reply to Redact.\n",
        "![title](img/AI_App_Policy_2.png)\n",
        "6. Wait for few seconds and re-run Step 7. In the access audits, you will see that the prompt is unchanged while the response is redacted.\n",
        "7. You can change the question in Step 7 and also try changing the policy to see the effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-xGC0Rq-l2e"
      },
      "source": [
        "# Next steps\n",
        "Continue your journey with PAIG by following these sections in the documentation link in the PAIG portal,\n",
        "- [Integrating with Langchain application](https://na.privacera.ai/docs/integration/langchain.html) \n",
        "- [Integrating with Python application](https://na.privacera.ai/docs/integration/python-applications.html)\n",
        "- [Try the Privacera SecureChat](https://na.privacera.ai/docs/integration/securechat.html)\n",
        "- [PAIG User Guide](https://na.privacera.ai/docs/user-guide/index.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
